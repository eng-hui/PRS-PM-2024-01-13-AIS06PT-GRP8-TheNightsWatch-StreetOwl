{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7103ff7a-b49a-4f5d-a2c2-81aa618ece1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import streamlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63a47020-bb99-4842-abc2-1b81def88442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 92.0ms\n",
      "Speed: 3.0ms preprocess, 92.0ms inference, 64.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 0.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.5ms\n",
      "Speed: 1.0ms preprocess, 6.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 1.0ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "# model = YOLO(\"yolov8n-seg.pt\")\n",
    "# model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"D:\\\\Downloads\\\\traffic.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True,classes=0)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1.0 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "\n",
    "        # Count the number of detected objects\n",
    "        num_objects = len(results[0].boxes)\n",
    "\n",
    "        # Put the FPS and object count on the frame\n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Detected: {num_objects}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Put the \"NUS-ISS Demo\" text in the top-right corner\n",
    "        text = \"NUS-ISS Demo\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        text_x = annotated_frame.shape[1] - text_size[0] - 10\n",
    "        text_y = 30\n",
    "        cv2.putText(annotated_frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            print(\"test\")\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        print(\"test\")\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "002ff685-9cd1-4f31-8c29-3a66b994f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 heads, 1 helmet, 1 person, 29.0ms\n",
      "Speed: 0.0ms preprocess, 29.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 persons, 2.0ms\n",
      "Speed: 2.6ms preprocess, 2.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 persons, 12.8ms\n",
      "Speed: 0.0ms preprocess, 12.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 persons, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 10.2ms\n",
      "Speed: 0.0ms preprocess, 10.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 0.2ms\n",
      "Speed: 3.9ms preprocess, 0.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 2 persons, 4.6ms\n",
      "Speed: 0.0ms preprocess, 4.6ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 2 persons, 9.2ms\n",
      "Speed: 0.0ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 2 persons, 9.6ms\n",
      "Speed: 8.6ms preprocess, 9.6ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 1 person, 9.9ms\n",
      "Speed: 4.0ms preprocess, 9.9ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 person, 9.5ms\n",
      "Speed: 4.1ms preprocess, 9.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 person, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 person, 7.9ms\n",
      "Speed: 0.4ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 10.2ms\n",
      "Speed: 0.0ms preprocess, 10.2ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.9ms\n",
      "Speed: 0.9ms preprocess, 12.9ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 9.5ms\n",
      "Speed: 3.5ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3.8ms\n",
      "Speed: 0.0ms preprocess, 3.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 8.8ms\n",
      "Speed: 0.0ms preprocess, 8.8ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 6.4ms\n",
      "Speed: 4.3ms preprocess, 6.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 4.2ms\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 3.8ms\n",
      "Speed: 0.0ms preprocess, 3.8ms inference, 8.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 12.9ms\n",
      "Speed: 0.0ms preprocess, 12.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 5.0ms\n",
      "Speed: 4.3ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 1 person, 6.1ms\n",
      "Speed: 3.7ms preprocess, 6.1ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 16.2ms\n",
      "Speed: 0.0ms preprocess, 16.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 11.3ms\n",
      "Speed: 0.0ms preprocess, 11.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 7.3ms\n",
      "Speed: 3.8ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 5.0ms\n",
      "Speed: 4.0ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 14.4ms\n",
      "Speed: 0.0ms preprocess, 14.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 4.1ms\n",
      "Speed: 0.0ms preprocess, 4.1ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 5.1ms\n",
      "Speed: 3.8ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 15.0ms\n",
      "Speed: 3.9ms preprocess, 15.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 9.8ms\n",
      "Speed: 0.0ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 6.3ms\n",
      "Speed: 4.3ms preprocess, 6.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 5.8ms\n",
      "Speed: 4.3ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 10.0ms\n",
      "Speed: 4.4ms preprocess, 10.0ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 11.1ms\n",
      "Speed: 0.0ms preprocess, 11.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 7.5ms\n",
      "Speed: 3.9ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 6.2ms\n",
      "Speed: 0.0ms preprocess, 6.2ms inference, 14.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 12.7ms\n",
      "Speed: 0.0ms preprocess, 12.7ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 7.8ms\n",
      "Speed: 0.0ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 4.3ms\n",
      "Speed: 4.0ms preprocess, 4.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 4.9ms\n",
      "Speed: 4.1ms preprocess, 4.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 9.9ms\n",
      "Speed: 0.0ms preprocess, 9.9ms inference, 11.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 11.5ms\n",
      "Speed: 0.0ms preprocess, 11.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 13.3ms\n",
      "Speed: 0.0ms preprocess, 13.3ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 5.2ms\n",
      "Speed: 3.9ms preprocess, 5.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 7.9ms\n",
      "Speed: 0.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 5.9ms\n",
      "Speed: 4.3ms preprocess, 5.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 8.6ms\n",
      "Speed: 8.4ms preprocess, 8.6ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 12.7ms\n",
      "Speed: 0.0ms preprocess, 12.7ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 8.7ms\n",
      "Speed: 4.0ms preprocess, 8.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 helmets, 1 person, 7.9ms\n",
      "Speed: 0.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 1 person, 5.9ms\n",
      "Speed: 0.3ms preprocess, 5.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 1 person, 11.7ms\n",
      "Speed: 4.2ms preprocess, 11.7ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 helmets, 1 person, 10.7ms\n",
      "Speed: 0.0ms preprocess, 10.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 1 person, 4.0ms\n",
      "Speed: 1.7ms preprocess, 4.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 1 person, 10.2ms\n",
      "Speed: 4.1ms preprocess, 10.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 1 person, 9.1ms\n",
      "Speed: 0.0ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 4 helmets, 1 person, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 4 helmets, 2 persons, 12.9ms\n",
      "Speed: 4.1ms preprocess, 12.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 4 helmets, 1 person, 6.9ms\n",
      "Speed: 4.3ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 4 helmets, 2 persons, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 10.2ms\n",
      "Speed: 3.9ms preprocess, 10.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 6.4ms\n",
      "Speed: 3.7ms preprocess, 6.4ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 5.9ms\n",
      "Speed: 4.1ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 8.0ms\n",
      "Speed: 0.0ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 12.4ms\n",
      "Speed: 8.6ms preprocess, 12.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 7.2ms\n",
      "Speed: 4.1ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 10.3ms\n",
      "Speed: 0.0ms preprocess, 10.3ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 9.3ms\n",
      "Speed: 4.3ms preprocess, 9.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 7.5ms\n",
      "Speed: 3.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 7.7ms\n",
      "Speed: 0.0ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.8ms\n",
      "Speed: 0.3ms preprocess, 4.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 3.4ms\n",
      "Speed: 2.2ms preprocess, 3.4ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 1.6ms\n",
      "Speed: 4.9ms preprocess, 1.6ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 11.2ms\n",
      "Speed: 4.2ms preprocess, 11.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 2 persons, 11.1ms\n",
      "Speed: 8.3ms preprocess, 11.1ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 7.7ms\n",
      "Speed: 4.1ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 4.4ms\n",
      "Speed: 3.7ms preprocess, 4.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 5.4ms\n",
      "Speed: 4.9ms preprocess, 5.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 helmets, 2 persons, 10.2ms\n",
      "Speed: 0.0ms preprocess, 10.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 6.2ms\n",
      "Speed: 4.2ms preprocess, 6.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 10.3ms\n",
      "Speed: 0.0ms preprocess, 10.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 10.9ms\n",
      "Speed: 0.0ms preprocess, 10.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 8.2ms\n",
      "Speed: 4.3ms preprocess, 8.2ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 12.1ms\n",
      "Speed: 0.0ms preprocess, 12.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 8.9ms\n",
      "Speed: 0.0ms preprocess, 8.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 4.9ms\n",
      "Speed: 0.0ms preprocess, 4.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 4.1ms\n",
      "Speed: 0.0ms preprocess, 4.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 helmets, 1 person, 14.2ms\n",
      "Speed: 0.0ms preprocess, 14.2ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 5.8ms\n",
      "Speed: 4.0ms preprocess, 5.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 6.1ms\n",
      "Speed: 4.2ms preprocess, 6.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 12.3ms\n",
      "Speed: 0.2ms preprocess, 12.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 3 helmets, 1 person, 11.8ms\n",
      "Speed: 0.0ms preprocess, 11.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 10.6ms\n",
      "Speed: 0.0ms preprocess, 10.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 5.9ms\n",
      "Speed: 4.4ms preprocess, 5.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 9.1ms\n",
      "Speed: 0.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 9.1ms\n",
      "Speed: 0.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 12.5ms\n",
      "Speed: 8.2ms preprocess, 12.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 12.8ms\n",
      "Speed: 0.0ms preprocess, 12.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 10.5ms\n",
      "Speed: 2.0ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 9.6ms\n",
      "Speed: 4.1ms preprocess, 9.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 18.8ms\n",
      "Speed: 0.0ms preprocess, 18.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 7.1ms\n",
      "Speed: 4.0ms preprocess, 7.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 6.3ms\n",
      "Speed: 3.7ms preprocess, 6.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 6.6ms\n",
      "Speed: 2.4ms preprocess, 6.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 2 helmets, 1 person, 5.1ms\n",
      "Speed: 4.0ms preprocess, 5.1ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 16.2ms\n",
      "Speed: 0.0ms preprocess, 16.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 9.2ms\n",
      "Speed: 4.1ms preprocess, 9.2ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.4ms\n",
      "Speed: 3.9ms preprocess, 8.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 20.9ms\n",
      "Speed: 0.0ms preprocess, 20.9ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.0ms\n",
      "Speed: 4.5ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.2ms\n",
      "Speed: 8.0ms preprocess, 8.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 7.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 10.8ms\n",
      "Speed: 4.1ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 15.0ms\n",
      "Speed: 0.0ms preprocess, 15.0ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 7.8ms\n",
      "Speed: 1.1ms preprocess, 7.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 5.8ms\n",
      "Speed: 4.5ms preprocess, 5.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 0.0ms\n",
      "Speed: 8.6ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 0.0ms\n",
      "Speed: 0.3ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.2ms\n",
      "Speed: 4.1ms preprocess, 4.2ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.2ms\n",
      "Speed: 4.1ms preprocess, 8.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 14.2ms\n",
      "Speed: 0.0ms preprocess, 14.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.3ms\n",
      "Speed: 3.8ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 9.7ms\n",
      "Speed: 0.0ms preprocess, 9.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 11.7ms\n",
      "Speed: 2.1ms preprocess, 11.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.0ms\n",
      "Speed: 4.2ms preprocess, 4.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.3ms\n",
      "Speed: 3.8ms preprocess, 4.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.2ms\n",
      "Speed: 0.0ms preprocess, 4.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 4.1ms\n",
      "Speed: 3.9ms preprocess, 4.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 12.5ms\n",
      "Speed: 1.8ms preprocess, 12.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 7.1ms\n",
      "Speed: 0.0ms preprocess, 7.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 7.6ms\n",
      "Speed: 4.3ms preprocess, 7.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 5.3ms\n",
      "Speed: 4.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 8.5ms\n",
      "Speed: 4.0ms preprocess, 8.5ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 helmet, 10.9ms\n",
      "Speed: 1.3ms preprocess, 10.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 12.2ms\n",
      "Speed: 4.0ms preprocess, 12.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 10.1ms\n",
      "Speed: 0.0ms preprocess, 10.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 2 helmets, 1 person, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 1 person, 7.9ms\n",
      "Speed: 0.0ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 1 person, 18.7ms\n",
      "Speed: 0.0ms preprocess, 18.7ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 1 person, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 1 person, 18.7ms\n",
      "Speed: 0.0ms preprocess, 18.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 9.9ms\n",
      "Speed: 0.0ms preprocess, 9.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 head, 1 helmet, 1 person, 3.0ms\n",
      "Speed: 1.9ms preprocess, 3.0ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 heads, 1 helmet, 1 person, 8.4ms\n",
      "Speed: 0.8ms preprocess, 8.4ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 2 persons, 18.0ms\n",
      "Speed: 0.0ms preprocess, 18.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 heads, 1 helmet, 2 persons, 8.6ms\n",
      "Speed: 3.6ms preprocess, 8.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import streamlink\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "# model = YOLO(\"yolov8n.pt\")\n",
    "# model = YOLO(\"yolov8l.pt\")\n",
    "# model = YOLO(\"yolov8n-obb.pt\")\n",
    "# model = YOLO(\"yolov8n-seg.pt\")\n",
    "# model = YOLO(\"yolov8l-seg.pt\")\n",
    "# model = YOLO(\"mybest.pt\")\n",
    "model = YOLO(\"hatbest.pt\")\n",
    "\n",
    "# Open Live stream\n",
    "# url = 'https://www.youtube.com/watch?v=DjdUEyjx8GM'\n",
    "url = 'https://www.youtube.com/watch?v=gFRtAAmiFbE'\n",
    "# url = 'https://www.youtube.com/watch?v=KY4Yd5QR570'\n",
    "\n",
    "# streams = streamlink.streams(url)\n",
    "# streams = streamlink.streams(url)\n",
    "# cap = cv2.VideoCapture(streams[\"720p\"].url)\n",
    "# cap = cv2.VideoCapture(streams[\"best\"].url)\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"short_video.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Get the video frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the counting lines (adjust these values as needed)\n",
    "left_line = int(frame_width * 0.2)  # 20% from the left\n",
    "right_line = int(frame_width * 0.8)  # 20% from the right\n",
    "\n",
    "# Counters for people exiting\n",
    "left_exit_count = 0\n",
    "right_exit_count = 0\n",
    "\n",
    "# Dictionary to store track histories\n",
    "track_history = {}\n",
    "\n",
    "# Sets to store IDs of people who have left\n",
    "left_exited_ids = set()\n",
    "right_exited_ids = set()\n",
    "\n",
    "# Function to check if a person has crossed a line\n",
    "def has_crossed_line(prev_pos, curr_pos, line_pos):\n",
    "    return (prev_pos < line_pos and curr_pos >= line_pos) or (prev_pos > line_pos and curr_pos <= line_pos)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        results = model.track(frame, persist=True, conf=0.1)  # class 0 is person\n",
    "        \n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot() # render annotated frame\n",
    "        # annotated_frame = frame.copy() # render empty frame\n",
    "\n",
    "        # Process each detected person\n",
    "        if results[0].masks is not None:\n",
    "            for mask in results[0].masks.xy:\n",
    "                # Convert the polygon to a format suitable for cv2.polylines\n",
    "                polygon = np.array(mask, dtype=np.int32)\n",
    "                \n",
    "                # Draw the polygon on the frame\n",
    "                # cv2.polylines(annotated_frame, [polygon], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "                cv2.fillPoly(annotated_frame, [polygon], color=(255, 0, 0, 127))  # semi-transparent fill\n",
    "        \n",
    "\n",
    "        # Draw the counting lines\n",
    "        cv2.line(annotated_frame, (left_line, 0), (left_line, frame_height), (255, 255, 0), 2)\n",
    "        cv2.line(annotated_frame, (right_line, 0), (right_line, frame_height), (255, 255, 0), 2)\n",
    "        \n",
    "        # Process each detected person\n",
    "        if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "            for box, track_id in zip(results[0].boxes.xywh, results[0].boxes.id):\n",
    "                x, y, w, h = box\n",
    "                track_id = int(track_id)\n",
    "                center_x, center_y = int(x), int(y)\n",
    "                \n",
    "                # Store the center points of each track\n",
    "                if track_id not in track_history:\n",
    "                    track_history[track_id] = []\n",
    "                track_history[track_id].append((center_x, center_y))\n",
    "                \n",
    "                # Keep only the last 30 positions\n",
    "                track_history[track_id] = track_history[track_id][-30:]\n",
    "                \n",
    "                # Check if the person has crossed a line, algo sucks....\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    prev_x = np.mean([pos[0] for pos in track_history[track_id][:-10]])\n",
    "                    curr_x = np.mean([pos[0] for pos in track_history[track_id][-10:]])\n",
    "                    \n",
    "                    if has_crossed_line(prev_x, curr_x, left_line) and track_id not in left_exited_ids:\n",
    "                        left_exit_count += 1\n",
    "                        left_exited_ids.add(track_id)\n",
    "                    elif has_crossed_line(prev_x, curr_x, right_line) and track_id not in right_exited_ids:\n",
    "                        right_exit_count += 1\n",
    "                        right_exited_ids.add(track_id)\n",
    "                \n",
    "                # Draw the track\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    cv2.polylines(annotated_frame, [np.array(track_history[track_id], dtype=np.int32)], False, (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw ID near the person\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1.0 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        \n",
    "        # Count the number of detected objects\n",
    "        num_objects = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        \n",
    "        # Put the FPS, object count, and exit counts on the frame\n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Detected: {num_objects}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Left exits: {left_exit_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Right exits: {right_exit_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Put the \"NUS-ISS Demo\" text in the top-right corner\n",
    "        text = \"NUS-ISS Demo\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        text_x = annotated_frame.shape[1] - text_size[0] - 10\n",
    "        text_y = 30\n",
    "        cv2.putText(annotated_frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9d54904-c7a0-49c3-a8ea-d1848c2fc8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masks[0].xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66cf0a05-8e99-4938-95f4-b1d49be3699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 persons, 42.0ms\n",
      "Speed: 2.7ms preprocess, 42.0ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 8.4ms\n",
      "Speed: 2.3ms preprocess, 8.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 36.1ms\n",
      "Speed: 0.0ms preprocess, 36.1ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 5.6ms\n",
      "Speed: 2.4ms preprocess, 5.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.8ms preprocess, 6.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.8ms\n",
      "Speed: 1.2ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i_kaz\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\i_kaz\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 1.3ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.1ms\n",
      "Speed: 0.0ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.9ms\n",
      "Speed: 4.4ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 10.4ms\n",
      "Speed: 0.0ms preprocess, 10.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.7ms\n",
      "Speed: 4.6ms preprocess, 7.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 4.3ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 15.2ms\n",
      "Speed: 0.0ms preprocess, 15.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.6ms\n",
      "Speed: 2.6ms preprocess, 6.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.2ms\n",
      "Speed: 0.0ms preprocess, 6.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 10.3ms\n",
      "Speed: 2.1ms preprocess, 10.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 4.7ms\n",
      "Speed: 4.0ms preprocess, 4.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.3ms\n",
      "Speed: 1.1ms preprocess, 7.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.4ms\n",
      "Speed: 5.6ms preprocess, 5.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.5ms\n",
      "Speed: 1.9ms preprocess, 8.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.9ms\n",
      "Speed: 0.4ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 3.6ms\n",
      "Speed: 3.0ms preprocess, 3.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.9ms\n",
      "Speed: 4.4ms preprocess, 5.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.3ms\n",
      "Speed: 4.0ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 9.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.1ms\n",
      "Speed: 0.3ms preprocess, 8.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 4.2ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 13.5ms\n",
      "Speed: 0.0ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 9.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.4ms\n",
      "Speed: 3.9ms preprocess, 6.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.4ms\n",
      "Speed: 3.6ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 3.8ms\n",
      "Speed: 4.5ms preprocess, 3.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.1ms\n",
      "Speed: 1.2ms preprocess, 8.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1.9ms\n",
      "Speed: 1.1ms preprocess, 1.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.5ms\n",
      "Speed: 0.2ms preprocess, 11.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 4.3ms preprocess, 12.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.2ms\n",
      "Speed: 3.9ms preprocess, 5.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.2ms\n",
      "Speed: 0.0ms preprocess, 10.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1.3ms\n",
      "Speed: 2.7ms preprocess, 1.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 4.0ms\n",
      "Speed: 3.7ms preprocess, 4.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.1ms\n",
      "Speed: 0.0ms preprocess, 6.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.6ms\n",
      "Speed: 0.0ms preprocess, 6.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.9ms\n",
      "Speed: 2.6ms preprocess, 8.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.5ms\n",
      "Speed: 3.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.2ms\n",
      "Speed: 4.2ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.9ms\n",
      "Speed: 4.1ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 5.0ms\n",
      "Speed: 2.3ms preprocess, 5.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.6ms\n",
      "Speed: 4.2ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.4ms\n",
      "Speed: 0.5ms preprocess, 5.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.7ms\n",
      "Speed: 1.0ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.4ms\n",
      "Speed: 0.0ms preprocess, 6.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.8ms\n",
      "Speed: 0.0ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 2.4ms\n",
      "Speed: 2.5ms preprocess, 2.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.0ms\n",
      "Speed: 1.8ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.7ms\n",
      "Speed: 2.5ms preprocess, 5.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.6ms\n",
      "Speed: 0.0ms preprocess, 5.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3.4ms\n",
      "Speed: 4.9ms preprocess, 3.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 3.9ms\n",
      "Speed: 0.0ms preprocess, 3.9ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 7.0ms\n",
      "Speed: 3.4ms preprocess, 7.0ms inference, 0.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.8ms\n",
      "Speed: 0.0ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.1ms\n",
      "Speed: 0.0ms preprocess, 6.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.9ms\n",
      "Speed: 0.1ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4.4ms\n",
      "Speed: 0.0ms preprocess, 4.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 7.8ms\n",
      "Speed: 0.0ms preprocess, 7.8ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.5ms\n",
      "Speed: 0.0ms preprocess, 5.5ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.6ms\n",
      "Speed: 0.0ms preprocess, 6.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.3ms\n",
      "Speed: 0.0ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 4.3ms\n",
      "Speed: 4.2ms preprocess, 4.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 4.2ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.5ms\n",
      "Speed: 4.6ms preprocess, 5.5ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.5ms\n",
      "Speed: 4.1ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.2ms\n",
      "Speed: 4.3ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.2ms\n",
      "Speed: 1.4ms preprocess, 10.2ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.7ms\n",
      "Speed: 0.0ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.3ms\n",
      "Speed: 0.0ms preprocess, 6.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.2ms\n",
      "Speed: 0.0ms preprocess, 12.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.4ms\n",
      "Speed: 4.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.8ms\n",
      "Speed: 4.1ms preprocess, 5.8ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.7ms\n",
      "Speed: 3.8ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.3ms\n",
      "Speed: 0.0ms preprocess, 10.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 11.4ms\n",
      "Speed: 0.0ms preprocess, 11.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.3ms\n",
      "Speed: 4.0ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.4ms\n",
      "Speed: 4.1ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.2ms\n",
      "Speed: 4.4ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3.0ms\n",
      "Speed: 1.1ms preprocess, 3.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.6ms\n",
      "Speed: 0.0ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.1ms\n",
      "Speed: 4.2ms preprocess, 6.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.8ms\n",
      "Speed: 4.2ms preprocess, 5.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3.8ms\n",
      "Speed: 0.0ms preprocess, 3.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.2ms\n",
      "Speed: 0.0ms preprocess, 6.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.6ms\n",
      "Speed: 10.1ms preprocess, 6.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 2.9ms\n",
      "Speed: 1.1ms preprocess, 2.9ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.4ms\n",
      "Speed: 0.0ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.2ms\n",
      "Speed: 4.1ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.1ms\n",
      "Speed: 4.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.1ms\n",
      "Speed: 0.0ms preprocess, 8.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4.3ms\n",
      "Speed: 0.0ms preprocess, 4.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 4.1ms\n",
      "Speed: 4.0ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 5.6ms\n",
      "Speed: 0.0ms preprocess, 5.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.6ms\n",
      "Speed: 0.0ms preprocess, 8.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.3ms\n",
      "Speed: 8.1ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.7ms\n",
      "Speed: 0.0ms preprocess, 8.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.9ms\n",
      "Speed: 0.0ms preprocess, 12.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 4.1ms preprocess, 6.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.1ms\n",
      "Speed: 4.1ms preprocess, 8.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.3ms\n",
      "Speed: 3.6ms preprocess, 6.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.2ms\n",
      "Speed: 4.2ms preprocess, 8.2ms inference, 8.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.7ms\n",
      "Speed: 0.5ms preprocess, 6.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 14.4ms\n",
      "Speed: 0.0ms preprocess, 14.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 4.3ms\n",
      "Speed: 0.0ms preprocess, 4.3ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.3ms\n",
      "Speed: 0.0ms preprocess, 9.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.5ms\n",
      "Speed: 0.0ms preprocess, 9.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.0ms\n",
      "Speed: 0.1ms preprocess, 6.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.4ms\n",
      "Speed: 4.2ms preprocess, 4.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.7ms\n",
      "Speed: 4.1ms preprocess, 6.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.4ms\n",
      "Speed: 3.7ms preprocess, 6.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.8ms\n",
      "Speed: 8.1ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.0ms\n",
      "Speed: 4.2ms preprocess, 6.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 10.4ms\n",
      "Speed: 0.0ms preprocess, 10.4ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.2ms\n",
      "Speed: 4.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.0ms\n",
      "Speed: 0.3ms preprocess, 5.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 12.2ms\n",
      "Speed: 0.0ms preprocess, 12.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 9.4ms\n",
      "Speed: 0.0ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.6ms\n",
      "Speed: 0.0ms preprocess, 6.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.0ms\n",
      "Speed: 4.2ms preprocess, 4.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.4ms\n",
      "Speed: 0.0ms preprocess, 6.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.9ms\n",
      "Speed: 4.0ms preprocess, 5.9ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.1ms\n",
      "Speed: 4.2ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.1ms\n",
      "Speed: 0.0ms preprocess, 4.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.9ms\n",
      "Speed: 0.0ms preprocess, 8.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.2ms\n",
      "Speed: 0.2ms preprocess, 6.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.0ms\n",
      "Speed: 0.0ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.4ms\n",
      "Speed: 0.0ms preprocess, 8.4ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.4ms\n",
      "Speed: 4.2ms preprocess, 4.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.1ms\n",
      "Speed: 4.3ms preprocess, 8.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.6ms\n",
      "Speed: 0.0ms preprocess, 5.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 9.9ms\n",
      "Speed: 0.0ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.7ms\n",
      "Speed: 6.9ms preprocess, 5.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.6ms\n",
      "Speed: 4.4ms preprocess, 5.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 10.2ms\n",
      "Speed: 0.2ms preprocess, 10.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 1.0ms\n",
      "Speed: 1.1ms preprocess, 1.0ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.6ms\n",
      "Speed: 0.0ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.7ms\n",
      "Speed: 0.0ms preprocess, 5.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 7.9ms\n",
      "Speed: 0.0ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.3ms\n",
      "Speed: 0.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.5ms\n",
      "Speed: 0.0ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 8.3ms\n",
      "Speed: 4.0ms preprocess, 8.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.0ms\n",
      "Speed: 3.9ms preprocess, 6.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 9.6ms\n",
      "Speed: 0.0ms preprocess, 9.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.0ms\n",
      "Speed: 4.0ms preprocess, 4.0ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.8ms\n",
      "Speed: 0.0ms preprocess, 5.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.3ms\n",
      "Speed: 0.0ms preprocess, 6.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 10.4ms\n",
      "Speed: 0.0ms preprocess, 10.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 4.3ms\n",
      "Speed: 4.1ms preprocess, 4.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 5.2ms\n",
      "Speed: 4.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 16.6ms\n",
      "Speed: 0.0ms preprocess, 16.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 9.9ms\n",
      "Speed: 0.0ms preprocess, 9.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 6.2ms\n",
      "Speed: 3.9ms preprocess, 6.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 3.4ms\n",
      "Speed: 2.6ms preprocess, 3.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.4ms\n",
      "Speed: 4.0ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 4.4ms\n",
      "Speed: 3.9ms preprocess, 4.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.1ms\n",
      "Speed: 0.9ms preprocess, 8.1ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 3.9ms\n",
      "Speed: 4.6ms preprocess, 3.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 5.5ms\n",
      "Speed: 0.6ms preprocess, 5.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 16.7ms\n",
      "Speed: 0.0ms preprocess, 16.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.3ms\n",
      "Speed: 0.0ms preprocess, 10.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import streamlink\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "# model = YOLO(\"yolov8l.pt\")\n",
    "# model = YOLO(\"yolov8x.pt\")\n",
    "# model = YOLO(\"yolov8n-obb.pt\")\n",
    "# model = YOLO(\"yolov8n-seg.pt\")\n",
    "# model = YOLO(\"yolov8l-seg.pt\")\n",
    "# model = YOLO(\"mybest.pt\")\n",
    "# model = YOLO(\"hatbest.pt\")\n",
    "# model = YOLO(\"my-best-segment.pt\")\n",
    "# model = YOLO(\"yolov10n.pt\")\n",
    "\n",
    "# Open Live stream\n",
    "url = 'https://www.youtube.com/watch?v=DjdUEyjx8GM'\n",
    "# url = 'https://www.youtube.com/watch?v=gFRtAAmiFbE'\n",
    "# url = 'https://www.youtube.com/watch?v=KY4Yd5QR570'\n",
    "\n",
    "# streams = streamlink.streams(url)\n",
    "streams = streamlink.streams(url)\n",
    "cap = cv2.VideoCapture(streams[\"720p\"].url)\n",
    "# cap = cv2.VideoCapture(streams[\"best\"].url)\n",
    "\n",
    "# Open the video file\n",
    "# video_path = \"short_video.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Get the video frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the counting lines (adjust these values as needed)\n",
    "left_line = int(frame_width * 0.2)  # 20% from the left\n",
    "right_line = int(frame_width * 0.8)  # 20% from the right\n",
    "\n",
    "# Counters for people exiting\n",
    "left_exit_count = 0\n",
    "right_exit_count = 0\n",
    "\n",
    "# Dictionary to store track histories\n",
    "track_history = {}\n",
    "\n",
    "# Sets to store IDs of people who have left\n",
    "left_exited_ids = set()\n",
    "right_exited_ids = set()\n",
    "\n",
    "# Function to check if a person has crossed a line\n",
    "def has_crossed_line(prev_pos, curr_pos, line_pos):\n",
    "    return (prev_pos < line_pos and curr_pos >= line_pos) or (prev_pos > line_pos and curr_pos <= line_pos)\n",
    "\n",
    "def smooth_polygon(polygon, smoothing=5):\n",
    "    \"\"\"Apply smoothing to the polygon points.\"\"\"\n",
    "    smooth_polygon = np.array(polygon, dtype=np.float32)\n",
    "    for _ in range(smoothing):\n",
    "        smooth_polygon = np.array([np.mean(np.roll(smooth_polygon, shift, axis=0), axis=0)\n",
    "                                   for shift in range(-1, 2)])\n",
    "    return np.array(smooth_polygon, dtype=np.int32)\n",
    "\n",
    "\n",
    "# Frame skip variable\n",
    "frame_skip = 0 # 2 is good\n",
    "frame_counter = 0\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # Skip frames to catch up with the stream\n",
    "    # frame_counter += 1\n",
    "    # if frame_skip > 0:\n",
    "    #     if frame_counter % frame_skip != 0:\n",
    "    #         continue\n",
    "    \n",
    "    if success:\n",
    "        \n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        # results = model.track(frame, persist=True, classes=0)  # class 0 is person\n",
    "        results = model.track(frame, persist=True, classes=0, conf=0.1,tracker=\"bytetrack.yaml\")  # class 0 is person, conf=0.25 sets the confidence threshold to 25%\n",
    "        # results = model.track(frame, persist=True, classes=0, conf=0.1)  # class 0 is person, conf=0.25 sets the confidence threshold to 25%\n",
    "        # results = model.track(frame, persist=True, conf=0.1)\n",
    "        # annotation\n",
    "        # frame = results[0].plot() # info from yolo v8, optional, can comment off\n",
    "        \n",
    "        # # Create a blank overlay for the semi-transparent masks\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        \n",
    "                                    # Process each detected person\n",
    "        if results[0].masks is not None:\n",
    "            for mask in results[0].masks.xy:\n",
    "                # Convert the polygon to a format suitable for cv2.fillPoly\n",
    "                polygon = np.array(mask, dtype=np.int32)\n",
    "                \n",
    "                # Fill the polygon on the overlay\n",
    "                cv2.fillPoly(overlay, [polygon], color=(255, 0, 255))  # Red color\n",
    "        \n",
    "        # Blend the overlay with the original frame\n",
    "        alpha = 0.3  # Adjust this value to change the transparency (0.0 - 1.0)\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "        # Draw the counting lines\n",
    "        cv2.line(annotated_frame, (left_line, 0), (left_line, frame_height), (255, 255, 0), 2)\n",
    "        cv2.line(annotated_frame, (right_line, 0), (right_line, frame_height), (255, 255, 0), 2)\n",
    "        \n",
    "        # Process each detected person\n",
    "        if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "            for box, track_id in zip(results[0].boxes.xywh, results[0].boxes.id):\n",
    "                x, y, w, h = box\n",
    "                track_id = int(track_id)\n",
    "                center_x, center_y = int(x), int(y)\n",
    "                \n",
    "                # Store the center points of each track\n",
    "                if track_id not in track_history:\n",
    "                    track_history[track_id] = []\n",
    "                track_history[track_id].append((center_x, center_y))\n",
    "                \n",
    "                # Keep only the last 30 positions\n",
    "                track_history[track_id] = track_history[track_id][-30:]\n",
    "                \n",
    "                # Check if the person has crossed a line, algo sucks....\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    prev_x = np.mean([pos[0] for pos in track_history[track_id][:-10]])\n",
    "                    curr_x = np.mean([pos[0] for pos in track_history[track_id][-10:]])\n",
    "                    \n",
    "                    if has_crossed_line(prev_x, curr_x, left_line) and track_id not in left_exited_ids:\n",
    "                        left_exit_count += 1\n",
    "                        left_exited_ids.add(track_id)\n",
    "                    elif has_crossed_line(prev_x, curr_x, right_line) and track_id not in right_exited_ids:\n",
    "                        right_exit_count += 1\n",
    "                        right_exited_ids.add(track_id)\n",
    "                \n",
    "                # Draw the track\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    cv2.polylines(annotated_frame, [np.array(track_history[track_id], dtype=np.int32)], False, (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw ID near the person\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1.0 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        \n",
    "        # Count the number of detected objects\n",
    "        num_objects = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        \n",
    "        # Put the FPS, object count, and exit counts on the frame\n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Detected: {num_objects}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Left exits: {left_exit_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Right exits: {right_exit_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Put the \"NUS-ISS Demo\" text in the top-right corner\n",
    "        text = \"NUS-ISS Demo\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        text_x = annotated_frame.shape[1] - text_size[0] - 10\n",
    "        text_y = 30\n",
    "        cv2.putText(annotated_frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc34715-6012-4928-8f04-f4eb2a219ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 28 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 43.0ms\n",
      "Speed: 2.0ms preprocess, 43.0ms inference, 16.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 7.0ms\n",
      "Speed: 63.9ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 traffic light, 2 backpacks, 1 umbrella, 3 handbags, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1 traffic light, 2 backpacks, 1 umbrella, 3 handbags, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 traffic light, 2 backpacks, 1 umbrella, 3 handbags, 1 suitcase, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 traffic light, 2 backpacks, 1 umbrella, 3 handbags, 1 suitcase, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 28 persons, 1 traffic light, 2 backpacks, 1 umbrella, 2 handbags, 1 suitcase, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1 traffic light, 2 backpacks, 1 umbrella, 1 handbag, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 traffic light, 3 backpacks, 2 handbags, 1 suitcase, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 traffic light, 3 backpacks, 4 handbags, 1 suitcase, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 traffic light, 2 backpacks, 1 umbrella, 4 handbags, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 1 traffic light, 2 backpacks, 1 umbrella, 4 handbags, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 traffic light, 2 backpacks, 1 umbrella, 1 handbag, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 backpack, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 handbag, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 handbag, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 handbag, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 handbag, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 12.0ms\n",
      "Speed: 1.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 21.0ms\n",
      "Speed: 2.0ms preprocess, 21.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 19.0ms\n",
      "Speed: 1.0ms preprocess, 19.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 1.0ms preprocess, 16.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 22.0ms\n",
      "Speed: 2.0ms preprocess, 22.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 18.0ms\n",
      "Speed: 4.0ms preprocess, 18.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 20.0ms\n",
      "Speed: 3.0ms preprocess, 20.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 3.0ms preprocess, 18.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 3.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 handbag, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 1.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.0ms\n",
      "Speed: 1.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.0ms\n",
      "Speed: 1.0ms preprocess, 10.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 19.0ms\n",
      "Speed: 2.0ms preprocess, 19.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 handbag, 19.0ms\n",
      "Speed: 3.0ms preprocess, 19.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 handbag, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.0ms\n",
      "Speed: 3.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 18.0ms\n",
      "Speed: 2.0ms preprocess, 18.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 2.0ms preprocess, 17.0ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 16.0ms\n",
      "Speed: 2.0ms preprocess, 16.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 15.0ms\n",
      "Speed: 1.0ms preprocess, 15.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 backpack, 15.0ms\n",
      "Speed: 5.0ms preprocess, 15.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 backpack, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 17.0ms\n",
      "Speed: 3.0ms preprocess, 17.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 15.0ms\n",
      "Speed: 2.0ms preprocess, 15.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.0ms\n",
      "Speed: 3.0ms preprocess, 14.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 6.0ms\n",
      "Speed: 1.0ms preprocess, 6.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 7.0ms\n",
      "Speed: 1.0ms preprocess, 7.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "import streamlink\n",
    "\n",
    "url = 'https://www.youtube.com/watch?v=gFRtAAmiFbE'\n",
    "streams = streamlink.streams(url)\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "model = YOLO(\"yolov8n-seg.pt\")  # segmentation model\n",
    "cap = cv2.VideoCapture(streams[\"720p\"].url)\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# out = cv2.VideoWriter(\"instance-segmentation-object-tracking.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    annotator = Annotator(im0, line_width=2)\n",
    "\n",
    "    results = model.track(im0, persist=True)\n",
    "\n",
    "    if results[0].boxes.id is not None and results[0].masks is not None:\n",
    "        masks = results[0].masks.xy\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "        for mask, track_id in zip(masks, track_ids):\n",
    "            color = colors(int(track_id), True)\n",
    "            txt_color = annotator.get_txt_color(color)\n",
    "            annotator.seg_bbox(mask=mask, mask_color=color, track_label=str(track_id))\n",
    "\n",
    "    # out.write(im0)\n",
    "    cv2.imshow(\"instance-segmentation-object-tracking\", im0)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be503f9-fcdc-41c1-868f-504f209b0755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method seg_bbox in module ultralytics.utils.plotting:\n",
      "\n",
      "seg_bbox(mask, mask_color=(255, 0, 255), det_label=None, track_label=None) method of ultralytics.utils.plotting.Annotator instance\n",
      "    Function for drawing segmented object in bounding box shape.\n",
      "    \n",
      "    Args:\n",
      "        mask (list): masks data list for instance segmentation area plotting\n",
      "        mask_color (tuple): mask foreground color\n",
      "        det_label (str): Detection label text\n",
      "        track_label (str): Tracking label text\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(annotator.seg_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e9eae4-c35a-4949-9eaf-a7a3c78372ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import streamlink\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "# model = YOLO(\"yolov8l.pt\")\n",
    "# model = YOLO(\"yolov8n-obb.pt\")\n",
    "model = YOLO(\"yolov8n-seg.pt\")\n",
    "# model = YOLO(\"yolov8l-seg.pt\")\n",
    "model = YOLO(\"mybest.pt\")\n",
    "model = YOLO(\"hatbest.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"D:\\\\Downloads\\\\traffic.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Get the video frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the counting lines (adjust these values as needed)\n",
    "left_line = int(frame_width * 0.2)  # 20% from the left\n",
    "right_line = int(frame_width * 0.8)  # 20% from the right\n",
    "\n",
    "# Counters for people exiting\n",
    "left_exit_count = 0\n",
    "right_exit_count = 0\n",
    "\n",
    "# Dictionary to store track histories\n",
    "track_history = {}\n",
    "\n",
    "# Sets to store IDs of people who have left\n",
    "left_exited_ids = set()\n",
    "right_exited_ids = set()\n",
    "\n",
    "# Function to check if a person has crossed a line\n",
    "def has_crossed_line(prev_pos, curr_pos, line_pos):\n",
    "    return (prev_pos < line_pos and curr_pos >= line_pos) or (prev_pos > line_pos and curr_pos <= line_pos)\n",
    "\n",
    "def smooth_polygon(polygon, smoothing=5):\n",
    "    \"\"\"Apply smoothing to the polygon points.\"\"\"\n",
    "    smooth_polygon = np.array(polygon, dtype=np.float32)\n",
    "    for _ in range(smoothing):\n",
    "        smooth_polygon = np.array([np.mean(np.roll(smooth_polygon, shift, axis=0), axis=0)\n",
    "                                   for shift in range(-1, 2)])\n",
    "    return np.array(smooth_polygon, dtype=np.int32)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        \n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        # results = model.track(frame, persist=True, classes=0)  # class 0 is person\n",
    "        results = model.track(frame, persist=True, classes=0, conf=0.1)  # class 0 is person, conf=0.25 sets the confidence threshold to 25%\n",
    "\n",
    "        # # Create a blank overlay for the semi-transparent masks\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        \n",
    "        # Process each detected person\n",
    "        if results[0].masks is not None:\n",
    "            for mask in results[0].masks.xy:\n",
    "                # Convert the polygon to a format suitable for cv2.fillPoly\n",
    "                polygon = np.array(mask, dtype=np.int32)\n",
    "                \n",
    "                # Fill the polygon on the overlay\n",
    "                cv2.fillPoly(overlay, [polygon], color=(255, 0, 255))  # Red color\n",
    "        \n",
    "        # Blend the overlay with the original frame\n",
    "        alpha = 0.3  # Adjust this value to change the transparency (0.0 - 1.0)\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "        # Draw the counting lines\n",
    "        cv2.line(annotated_frame, (left_line, 0), (left_line, frame_height), (255, 255, 0), 2)\n",
    "        cv2.line(annotated_frame, (right_line, 0), (right_line, frame_height), (255, 255, 0), 2)\n",
    "        \n",
    "        # Process each detected person\n",
    "        if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "            for box, track_id in zip(results[0].boxes.xywh, results[0].boxes.id):\n",
    "                x, y, w, h = box\n",
    "                track_id = int(track_id)\n",
    "                center_x, center_y = int(x), int(y)\n",
    "                \n",
    "                # Store the center points of each track\n",
    "                if track_id not in track_history:\n",
    "                    track_history[track_id] = []\n",
    "                track_history[track_id].append((center_x, center_y))\n",
    "                \n",
    "                # Keep only the last 30 positions\n",
    "                track_history[track_id] = track_history[track_id][-30:]\n",
    "                \n",
    "                # Check if the person has crossed a line, algo sucks....\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    prev_x = np.mean([pos[0] for pos in track_history[track_id][:-10]])\n",
    "                    curr_x = np.mean([pos[0] for pos in track_history[track_id][-10:]])\n",
    "                    \n",
    "                    if has_crossed_line(prev_x, curr_x, left_line) and track_id not in left_exited_ids:\n",
    "                        left_exit_count += 1\n",
    "                        left_exited_ids.add(track_id)\n",
    "                    elif has_crossed_line(prev_x, curr_x, right_line) and track_id not in right_exited_ids:\n",
    "                        right_exit_count += 1\n",
    "                        right_exited_ids.add(track_id)\n",
    "                \n",
    "                # Draw the track\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    cv2.polylines(annotated_frame, [np.array(track_history[track_id], dtype=np.int32)], False, (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw ID near the person\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1.0 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        \n",
    "        # Count the number of detected objects\n",
    "        num_objects = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        \n",
    "        # Put the FPS, object count, and exit counts on the frame\n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Detected: {num_objects}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Left exits: {left_exit_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Right exits: {right_exit_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Put the \"NUS-ISS Demo\" text in the top-right corner\n",
    "        text = \"NUS-ISS Demo\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        text_x = annotated_frame.shape[1] - text_size[0] - 10\n",
    "        text_y = 30\n",
    "        cv2.putText(annotated_frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c69a9241-4a5d-490a-a10a-3bc8faa2c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.72 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.50  Python-3.10.6 torch-2.3.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=coco8-seg.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\segment\\train2\n",
      "\n",
      "Dataset 'coco8-seg.yaml' images not found , missing path 'C:\\Users\\i_kaz\\datasets\\coco8-seg\\images\\val'\n",
      "Downloading https://ultralytics.com/assets/coco8-seg.zip to 'C:\\Users\\i_kaz\\datasets\\coco8-seg.zip'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 439k/439k [00:00<00:00, 32.1MB/s]\n",
      "Unzipping C:\\Users\\i_kaz\\datasets\\coco8-seg.zip to C:\\Users\\i_kaz\\datasets\\coco8-seg...: 100%|| 25/25 [00:00<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download success  (2.8s), saved to \u001b[1mC:\\Users\\i_kaz\\datasets\u001b[0m\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n-seg summary: 261 layers, 3409968 parameters, 3409952 gradients, 12.8 GFLOPs\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\segment\\train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\i_kaz\\datasets\\coco8-seg\\labels\\train... 4 images, 0 backgrounds, 0 corrupt: 100%|| \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\i_kaz\\datasets\\coco8-seg\\labels\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\i_kaz\\datasets\\coco8-seg\\labels\\val... 4 images, 0 backgrounds, 0 corrupt: 100%|| 4/4 \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\i_kaz\\datasets\\coco8-seg\\labels\\val.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\segment\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/08/04 10:58:01 info mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2024/08/04 10:58:01 info mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(288f4758984744f595c36dab628d72f7) to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs\\mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING  Failed to initialize: Changing param values is not allowed. Param with key='data' was already logged with value='C:\\Users\\i_kaz\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\ultralytics\\cfg\\datasets\\coco8.yaml' for run ID='288f4758984744f595c36dab628d72f7'. Attempted logging new value 'C:\\Users\\i_kaz\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\ultralytics\\cfg\\datasets\\coco8-seg.yaml'.\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mWARNING  Not tracking this run\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns\\segment\\train2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100      1.71G     0.8897      2.566      3.343       1.22         22        640: 100%|| 1/1 [00:01<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.684      0.633      0.747      0.492      0.673      0.617      0.631      0.443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100      1.61G       1.12      3.675      2.697      1.308         37        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.685      0.633      0.748      0.485      0.665        0.6      0.618      0.438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100      1.61G      1.024      2.666      2.088       1.21         21        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.693      0.633       0.75      0.501      0.672        0.6      0.619      0.439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100      1.61G     0.7543       1.61      2.742      1.087         22        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.699      0.633      0.744      0.513      0.687      0.617      0.632      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100      1.64G     0.9388      2.476      3.084      1.275         20        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.704      0.633      0.743      0.511      0.692      0.617      0.632       0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100      1.64G     0.8144      2.761      2.156      1.122         22        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.712      0.633      0.798       0.52      0.642      0.517        0.6      0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100      1.63G     0.9878      2.483       2.96      1.431         22        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.709      0.633      0.805      0.518      0.636      0.517      0.602      0.431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      8/100      1.63G      1.056      2.695      3.637      1.259         24        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.71      0.633      0.826      0.558      0.678        0.6      0.747      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      9/100      1.63G     0.7462      2.309      2.914      1.122         23        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.712      0.617      0.825      0.566      0.677        0.6      0.747      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     10/100      1.63G     0.9524       2.95      2.538      1.288         25        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.713      0.617      0.922      0.595      0.684        0.6      0.829      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     11/100      1.63G      0.986      2.508      2.475       1.32         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.706      0.617      0.916      0.584      0.688      0.692      0.854      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     12/100      1.63G     0.7736      2.577      2.089      1.171         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.706      0.617      0.915      0.583      0.684        0.6      0.853      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     13/100      1.63G     0.9234      3.104      2.838      1.277         26        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.694      0.617      0.901      0.605      0.683        0.6      0.852      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     14/100      1.64G      0.756      2.816      2.392      1.123         17        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.704      0.617      0.893      0.619      0.692        0.6      0.854      0.526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     15/100      1.63G      1.132      3.139      2.802       1.41         38        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.709      0.617      0.891      0.589      0.697        0.6      0.853      0.511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     16/100      1.63G      1.018      2.559      1.971      1.327         49        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.688      0.617      0.891      0.551      0.677        0.6      0.798       0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     17/100      1.64G     0.8384      2.517      1.465       1.16         25        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.683      0.617      0.886       0.56      0.673        0.6      0.848      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     18/100      1.64G     0.7205      1.783      1.327     0.9884         17        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.683      0.617      0.886       0.56      0.673        0.6      0.848      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     19/100      1.64G     0.7243      2.382      1.532      1.009         34        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.676      0.617       0.88      0.564      0.666        0.6      0.847      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     20/100      1.63G       1.16      2.684      1.663      1.308         25        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.676      0.617       0.88      0.564      0.666        0.6      0.847      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     21/100      1.64G     0.7482       1.89      2.012      1.119         26        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.661      0.617      0.873      0.571      0.653        0.6      0.848      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     22/100      1.63G     0.9132      2.508      1.531      1.138         53        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.661      0.617      0.873      0.571      0.653        0.6      0.848      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     23/100      1.65G     0.8147      2.588      1.502      1.126         22        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.661      0.617      0.858      0.567      0.653        0.6      0.845      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     24/100      1.63G     0.9731       2.25       1.52      1.225         34        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.661      0.617      0.858      0.567      0.653        0.6      0.845      0.492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     25/100      1.65G      0.786      2.608      1.721      1.119         11        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.675      0.617      0.853      0.582      0.667        0.6      0.844      0.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     26/100      1.63G     0.6303      2.004      1.053      1.039         35        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.675      0.617      0.853      0.582      0.667        0.6      0.844      0.527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     27/100      1.65G     0.7569      2.074      1.324       1.16         25        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.934        0.4      0.853       0.55      0.934        0.4      0.843      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     28/100      1.63G     0.7395       2.07     0.9258      1.079         35        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.934        0.4      0.853       0.55      0.934        0.4      0.843      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     29/100      1.64G     0.9138      2.223      1.276      1.174         28        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.929        0.4      0.855      0.552      0.929        0.4      0.844      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     30/100      1.63G     0.8287      1.849      1.083      1.098         29        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.929        0.4      0.855      0.552      0.929        0.4      0.844      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     31/100      1.64G     0.9143      2.564      1.557      1.215         27        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927        0.4      0.855      0.553      0.927        0.4      0.849      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     32/100      1.63G     0.6848      2.128      1.305       1.17         20        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927        0.4      0.855      0.553      0.927        0.4      0.849      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     33/100      1.65G      1.063      1.999      1.347      1.266         32        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927      0.397      0.816      0.512      0.927      0.397      0.808      0.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     34/100      1.63G     0.9068      1.964       1.49      1.183         33        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927      0.397      0.816      0.512      0.927      0.397      0.808      0.496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     35/100      1.64G     0.8056       1.68      1.018      1.107         24        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.925      0.393      0.811      0.513      0.925      0.393      0.805      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     36/100      1.63G      0.826      2.709      1.413      1.208         34        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.925      0.393      0.811      0.513      0.925      0.393      0.805      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     37/100      1.65G     0.7955      2.259       1.07      1.121         34        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.928      0.387      0.642      0.384      0.848      0.311      0.513      0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     38/100      1.63G     0.5842      1.625     0.7595     0.9696         29        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.928      0.387      0.642      0.384      0.848      0.311      0.513      0.355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     39/100      1.65G     0.5598      1.688     0.6864      1.012         27        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927      0.381      0.646      0.384      0.846        0.3      0.517      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     40/100      1.64G     0.6375      1.561      1.075     0.9945         21        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.927      0.381      0.646      0.384      0.846        0.3      0.517      0.357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     41/100      1.64G      0.974      1.769       1.09      1.201         30        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.932      0.383      0.594      0.347       0.76      0.317       0.46      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     42/100      1.63G     0.6695      1.661      1.021     0.9869         36        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.932      0.383      0.594      0.347       0.76      0.317       0.46      0.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     43/100      1.65G     0.7116      1.727       1.03      1.152         20        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.928      0.383      0.567      0.328      0.747      0.317      0.423      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     44/100      1.64G     0.6821      1.889     0.9115      1.109         23        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.928      0.383      0.567      0.328      0.747      0.317      0.423      0.293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     45/100      1.64G     0.6151      1.958      1.209      1.047         41        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.931      0.388       0.55       0.32      0.873      0.379      0.527      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     46/100      1.63G     0.7278      1.787       1.03      1.092         24        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.931      0.388       0.55       0.32      0.873      0.379      0.527      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     47/100      1.65G     0.7324      1.662      1.001       1.07         16        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.932      0.383      0.535      0.307      0.739      0.317      0.394      0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     48/100      1.63G     0.7018      1.608      0.888      1.061         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.932      0.383      0.535      0.307      0.739      0.317      0.394      0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     49/100      1.65G     0.6996      1.875     0.9725      1.147         19        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.921      0.383      0.527      0.298      0.909      0.357      0.514      0.266\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     50/100      1.63G      0.612      1.578     0.8192      1.024         33        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.921      0.383      0.527      0.298      0.909      0.357      0.514      0.266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     51/100      1.64G     0.6302      1.813     0.8377       1.08         36        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.918      0.383      0.469      0.264      0.908      0.358      0.456      0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     52/100      1.63G      0.698       2.42      1.447      1.078         55        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.918      0.383      0.469      0.264      0.908      0.358      0.456      0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     53/100      1.65G      0.402      1.457     0.7205     0.9053         15        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.918      0.383      0.469      0.264      0.908      0.358      0.456      0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     54/100      1.64G     0.6115      1.697     0.9377     0.9817         47        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.923      0.392      0.465      0.262      0.869      0.378      0.451      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     55/100      1.63G     0.6327      1.559      1.004      1.068         32        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.923      0.392      0.465      0.262      0.869      0.378      0.451      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     56/100      1.64G     0.5861      1.409      0.783      1.046         23        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.923      0.392      0.465      0.262      0.869      0.378      0.451      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     57/100      1.64G       0.54      1.453     0.6793       1.01         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.911      0.383      0.443      0.253      0.874      0.374       0.43       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     58/100      1.64G     0.6962      1.288      1.042      1.075         11        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.911      0.383      0.443      0.253      0.874      0.374       0.43       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     59/100      1.64G     0.6611      1.603     0.9954      1.059         20        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.911      0.383      0.443      0.253      0.874      0.374       0.43       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     60/100      1.65G     0.4828      1.467      0.837     0.9941         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.919      0.381      0.457      0.267      0.869      0.378      0.424      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     61/100      1.64G     0.5758      1.296     0.6102      1.002         26        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.919      0.381      0.457      0.267      0.869      0.378      0.424      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     62/100      1.65G     0.5622      1.474     0.6987      1.067         20        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.919      0.381      0.457      0.267      0.869      0.378      0.424      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     63/100      1.65G     0.4184      1.328     0.6824     0.8912         17        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.914      0.383      0.456      0.253      0.831      0.367      0.422      0.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     64/100      1.63G     0.6163      1.471     0.7651     0.9701         27        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.914      0.383      0.456      0.253      0.831      0.367      0.422      0.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     65/100      1.64G     0.6374      1.506     0.6162      1.049         37        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.914      0.383      0.456      0.253      0.831      0.367      0.422      0.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     66/100      1.64G     0.8057      2.239      1.766      1.175         52        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383      0.428      0.254       0.86      0.367      0.411      0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     67/100      1.63G     0.5219      1.696     0.6938     0.9517         37        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383      0.428      0.254       0.86      0.367      0.411      0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     68/100      1.64G     0.6069      1.621     0.6854     0.9698         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383      0.428      0.254       0.86      0.367      0.411      0.223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     69/100      1.65G     0.5653      1.432      1.021      1.127         12        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383       0.44      0.242       0.75      0.317      0.301      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     70/100      1.64G     0.8123      1.821      1.624      1.092         10        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383       0.44      0.242       0.75      0.317      0.301      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     71/100      1.65G     0.5146      1.141      0.558     0.9974         23        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383       0.44      0.242       0.75      0.317      0.301      0.206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     72/100      1.65G     0.4442      1.479     0.5787     0.9684         22        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383      0.424      0.275      0.859      0.367      0.409      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     73/100      1.64G     0.4959      1.198      0.557     0.9598         16        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383      0.424      0.275      0.859      0.367      0.409      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     74/100      1.64G     0.7471      2.016      1.286      1.185         28        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913      0.383      0.424      0.275      0.859      0.367      0.409      0.222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     75/100      1.64G     0.8377      1.986     0.8301      1.111         60        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.916      0.383      0.426      0.238      0.757      0.317      0.289      0.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     76/100      1.63G     0.6292      1.541     0.6414      1.004         17        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.916      0.383      0.426      0.238      0.757      0.317      0.289      0.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     77/100      1.64G     0.5362      1.358     0.6382     0.9377         37        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.916      0.383      0.426      0.238      0.757      0.317      0.289      0.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     78/100      1.64G     0.5792      1.562     0.7852      1.006         45        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.915      0.383      0.425      0.233      0.755      0.308      0.287      0.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     79/100      1.64G     0.7389      1.677      1.088      1.066         17        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.915      0.383      0.425      0.233      0.755      0.308      0.287      0.198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     80/100      1.65G     0.4502      1.201     0.6517     0.9772         23        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.915      0.383      0.425      0.233      0.755      0.308      0.287      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     81/100      1.65G     0.5991      1.504      0.589     0.9909         42        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383       0.42      0.255      0.782       0.29      0.328      0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     82/100      1.63G     0.4365      1.189     0.4551     0.9002         32        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383       0.42      0.255      0.782       0.29      0.328      0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     83/100      1.64G     0.5055      1.247     0.5236     0.9488         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.912      0.383       0.42      0.255      0.782       0.29      0.328      0.214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     84/100      1.64G     0.5608      1.751      1.153      1.059         33        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.91      0.383      0.421       0.28      0.852      0.368       0.41      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     85/100      1.63G      0.477      1.329     0.6218     0.9712         27        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.91      0.383      0.421       0.28      0.852      0.368       0.41      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     86/100      1.65G     0.5663       1.29     0.7516      1.031         29        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.91      0.383      0.421       0.28      0.852      0.368       0.41      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     87/100      1.65G     0.4601      1.336     0.5772     0.9338         32        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17       0.91      0.383      0.421       0.28      0.852      0.368       0.41      0.231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     88/100      1.64G     0.6353      1.599     0.9078       1.03         47        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.902      0.383      0.421      0.256      0.778      0.299      0.328      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     89/100      1.63G     0.5939      1.334     0.7253       1.04         28        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.902      0.383      0.421      0.256      0.778      0.299      0.328      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     90/100      1.65G     0.6035       1.66     0.6744      1.015         31        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.902      0.383      0.421      0.256      0.778      0.299      0.328      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     91/100      1.65G     0.6111      1.285     0.5824      1.077         13        640: 100%|| 1/1 [00:04<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.902      0.383      0.421      0.256      0.778      0.299      0.328      0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     92/100      1.65G     0.4725      1.191     0.5161     0.8767         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.895      0.383      0.427      0.256      0.774      0.283      0.328      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     93/100      1.64G     0.4971      1.382     0.5232     0.9208         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.895      0.383      0.427      0.256      0.774      0.283      0.328      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     94/100      1.65G      0.596      1.446     0.6026      1.038         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.895      0.383      0.427      0.256      0.774      0.283      0.328      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     95/100      1.65G     0.7353      1.404     0.7613      1.145         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.895      0.383      0.427      0.256      0.774      0.283      0.328      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     96/100      1.65G     0.9035      1.706      0.813      1.317         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.907      0.383      0.507      0.295      0.785      0.283      0.423      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     97/100      1.64G     0.5675      1.333      0.564     0.9238         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.907      0.383      0.507      0.295      0.785      0.283      0.423      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     98/100      1.65G     0.5176      1.364      0.662     0.9909         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.907      0.383      0.507      0.295      0.785      0.283      0.423      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     99/100      1.65G     0.5552      1.149     0.6036     0.9996         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.907      0.383      0.507      0.295      0.785      0.283      0.423      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    100/100      1.65G     0.4824       1.15     0.5965     0.9618         13        640: 100%|| 1/1 [00:00<0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.913       0.38      0.511      0.309      0.778       0.28       0.37      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100 epochs completed in 0.026 hours.\n",
      "Optimizer stripped from runs\\segment\\train2\\weights\\last.pt, 7.1MB\n",
      "Optimizer stripped from runs\\segment\\train2\\weights\\best.pt, 7.1MB\n",
      "\n",
      "Validating runs\\segment\\train2\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.50  Python-3.10.6 torch-2.3.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3404320 parameters, 0 gradients, 12.6 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4         17      0.696      0.617      0.893       0.62      0.684        0.6      0.854       0.53\n",
      "                person          3         10      0.512        0.7      0.681      0.318      0.439        0.6      0.618      0.253\n",
      "                   dog          1          1      0.807          1      0.995      0.895      0.807          1      0.995      0.895\n",
      "                 horse          1          2      0.305          1      0.995      0.598      0.305          1      0.995      0.324\n",
      "              elephant          1          2          1          0      0.695      0.366          1          0      0.523      0.314\n",
      "              umbrella          1          1      0.551          1      0.995      0.895      0.551          1      0.995      0.895\n",
      "          potted plant          1          1          1          0      0.995      0.647          1          0      0.995      0.497\n",
      "Speed: 0.3ms preprocess, 5.7ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\segment\\train2\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td></td></tr><tr><td>lr/pg1</td><td></td></tr><tr><td>lr/pg2</td><td></td></tr><tr><td>metrics/mAP50(B)</td><td></td></tr><tr><td>metrics/mAP50(M)</td><td></td></tr><tr><td>metrics/mAP50-95(B)</td><td></td></tr><tr><td>metrics/mAP50-95(M)</td><td></td></tr><tr><td>metrics/precision(B)</td><td></td></tr><tr><td>metrics/precision(M)</td><td></td></tr><tr><td>metrics/recall(B)</td><td></td></tr><tr><td>metrics/recall(M)</td><td></td></tr><tr><td>model/GFLOPs</td><td></td></tr><tr><td>model/parameters</td><td></td></tr><tr><td>model/speed_PyTorch(ms)</td><td></td></tr><tr><td>train/box_loss</td><td></td></tr><tr><td>train/cls_loss</td><td></td></tr><tr><td>train/dfl_loss</td><td></td></tr><tr><td>train/seg_loss</td><td></td></tr><tr><td>val/box_loss</td><td></td></tr><tr><td>val/cls_loss</td><td></td></tr><tr><td>val/dfl_loss</td><td></td></tr><tr><td>val/seg_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.0</td></tr><tr><td>lr/pg1</td><td>0.0</td></tr><tr><td>lr/pg2</td><td>0.0</td></tr><tr><td>metrics/mAP50(B)</td><td>0.89265</td></tr><tr><td>metrics/mAP50(M)</td><td>0.85357</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.61987</td></tr><tr><td>metrics/mAP50-95(M)</td><td>0.52993</td></tr><tr><td>metrics/precision(B)</td><td>0.69576</td></tr><tr><td>metrics/precision(M)</td><td>0.68357</td></tr><tr><td>metrics/recall(B)</td><td>0.61667</td></tr><tr><td>metrics/recall(M)</td><td>0.6</td></tr><tr><td>model/GFLOPs</td><td>12.773</td></tr><tr><td>model/parameters</td><td>3409968</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>61.499</td></tr><tr><td>train/box_loss</td><td>0.48235</td></tr><tr><td>train/cls_loss</td><td>0.5965</td></tr><tr><td>train/dfl_loss</td><td>0.96177</td></tr><tr><td>train/seg_loss</td><td>1.1503</td></tr><tr><td>val/box_loss</td><td>1.71028</td></tr><tr><td>val/cls_loss</td><td>2.10219</td></tr><tr><td>val/dfl_loss</td><td>1.41726</td></tr><tr><td>val/seg_loss</td><td>2.81193</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">train</strong> at: <a href='https://wandb.ai/sendtokazy-nus/YOLOv8/runs/dc3mykss' target=\"_blank\">https://wandb.ai/sendtokazy-nus/YOLOv8/runs/dc3mykss</a><br/>Synced 5 W&B file(s), 28 media file(s), 9 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240804_105652-dc3mykss\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n-seg\")\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=\"coco8-seg.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "461776e9-0558-4889-a1ef-52853160320a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method track in module ultralytics.engine.model:\n",
      "\n",
      "track(source: Union[str, pathlib.Path, int, list, tuple, numpy.ndarray, torch.Tensor] = None, stream: bool = False, persist: bool = False, **kwargs) -> List[ultralytics.engine.results.Results] method of ultralytics.models.yolo.model.YOLO instance\n",
      "    Conducts object tracking on the specified input source using the registered trackers.\n",
      "    \n",
      "    This method performs object tracking using the model's predictors and optionally registered trackers. It is\n",
      "    capable of handling different types of input sources such as file paths or video streams. The method supports\n",
      "    customization of the tracking process through various keyword arguments. It registers trackers if they are not\n",
      "    already present and optionally persists them based on the 'persist' flag.\n",
      "    \n",
      "    The method sets a default confidence threshold specifically for ByteTrack-based tracking, which requires low\n",
      "    confidence predictions as input. The tracking mode is explicitly set in the keyword arguments.\n",
      "    \n",
      "    Args:\n",
      "        source (str, optional): The input source for object tracking. It can be a file path, URL, or video stream.\n",
      "        stream (bool, optional): Treats the input source as a continuous video stream. Defaults to False.\n",
      "        persist (bool, optional): Persists the trackers between different calls to this method. Defaults to False.\n",
      "        **kwargs (any): Additional keyword arguments for configuring the tracking process. These arguments allow\n",
      "            for further customization of the tracking behavior.\n",
      "    \n",
      "    Returns:\n",
      "        (List[ultralytics.engine.results.Results]): A list of tracking results, encapsulated in the Results class.\n",
      "    \n",
      "    Raises:\n",
      "        AttributeError: If the predictor does not have registered trackers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.track)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
