{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e18f590-ae39-459a-a04d-16d8f07af5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 7 persons, 18.0ms\n",
      "Speed: 2.9ms preprocess, 18.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 17.1ms\n",
      "Speed: 1.1ms preprocess, 17.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 19.2ms\n",
      "Speed: 0.0ms preprocess, 19.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 14.6ms\n",
      "Speed: 2.1ms preprocess, 14.6ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 16.4ms\n",
      "Speed: 1.7ms preprocess, 16.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 16.5ms\n",
      "Speed: 1.5ms preprocess, 16.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 16.4ms\n",
      "Speed: 1.0ms preprocess, 16.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\i_kaz\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\i_kaz\\anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 8 persons, 15.2ms\n",
      "Speed: 0.0ms preprocess, 15.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.9ms\n",
      "Speed: 1.0ms preprocess, 16.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.7ms\n",
      "Speed: 4.2ms preprocess, 16.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 4.2ms preprocess, 12.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.9ms\n",
      "Speed: 0.7ms preprocess, 12.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.3ms\n",
      "Speed: 0.0ms preprocess, 12.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 15.4ms\n",
      "Speed: 0.0ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.6ms\n",
      "Speed: 0.0ms preprocess, 16.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.9ms\n",
      "Speed: 0.0ms preprocess, 12.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 4.2ms preprocess, 12.4ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.9ms\n",
      "Speed: 3.6ms preprocess, 11.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.9ms\n",
      "Speed: 4.1ms preprocess, 12.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.8ms\n",
      "Speed: 4.2ms preprocess, 12.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 10.4ms\n",
      "Speed: 0.9ms preprocess, 10.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 16.4ms\n",
      "Speed: 0.0ms preprocess, 16.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.5ms\n",
      "Speed: 1.6ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.6ms\n",
      "Speed: 4.1ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 13.3ms\n",
      "Speed: 0.0ms preprocess, 13.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 11.8ms\n",
      "Speed: 0.3ms preprocess, 11.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.1ms\n",
      "Speed: 0.0ms preprocess, 12.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 12.4ms\n",
      "Speed: 1.6ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 4.1ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.6ms\n",
      "Speed: 1.2ms preprocess, 11.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.6ms\n",
      "Speed: 0.0ms preprocess, 16.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.4ms\n",
      "Speed: 0.0ms preprocess, 16.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.7ms\n",
      "Speed: 1.2ms preprocess, 12.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 10.4ms\n",
      "Speed: 1.1ms preprocess, 10.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 4.1ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 13.1ms\n",
      "Speed: 1.7ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 8.2ms\n",
      "Speed: 0.0ms preprocess, 8.2ms inference, 9.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.3ms\n",
      "Speed: 1.0ms preprocess, 12.3ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.3ms\n",
      "Speed: 1.4ms preprocess, 11.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.7ms\n",
      "Speed: 1.6ms preprocess, 12.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 3.9ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.6ms\n",
      "Speed: 4.0ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.6ms\n",
      "Speed: 3.9ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.7ms\n",
      "Speed: 0.0ms preprocess, 12.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 1.1ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.8ms\n",
      "Speed: 3.8ms preprocess, 12.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.7ms\n",
      "Speed: 0.0ms preprocess, 16.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.1ms\n",
      "Speed: 0.0ms preprocess, 14.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 4.1ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.4ms\n",
      "Speed: 1.1ms preprocess, 11.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 4.2ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.3ms\n",
      "Speed: 0.0ms preprocess, 14.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.6ms\n",
      "Speed: 0.0ms preprocess, 14.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 13.7ms\n",
      "Speed: 4.4ms preprocess, 13.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 1.3ms preprocess, 12.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.7ms\n",
      "Speed: 4.0ms preprocess, 12.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.6ms\n",
      "Speed: 1.0ms preprocess, 11.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.1ms\n",
      "Speed: 4.3ms preprocess, 12.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.6ms\n",
      "Speed: 1.0ms preprocess, 11.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 3.9ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 15.8ms\n",
      "Speed: 0.0ms preprocess, 15.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.3ms\n",
      "Speed: 0.3ms preprocess, 12.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.1ms\n",
      "Speed: 1.8ms preprocess, 12.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 4.1ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.6ms\n",
      "Speed: 1.1ms preprocess, 11.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 14.3ms\n",
      "Speed: 0.8ms preprocess, 14.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 4.0ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.5ms\n",
      "Speed: 4.1ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 13.0ms\n",
      "Speed: 4.2ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 4.1ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 13.6ms\n",
      "Speed: 4.1ms preprocess, 13.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.6ms\n",
      "Speed: 1.1ms preprocess, 12.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 11.2ms\n",
      "Speed: 1.3ms preprocess, 11.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 11.3ms\n",
      "Speed: 1.2ms preprocess, 11.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 13.5ms\n",
      "Speed: 4.0ms preprocess, 13.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.7ms\n",
      "Speed: 4.0ms preprocess, 12.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 14.5ms\n",
      "Speed: 0.0ms preprocess, 14.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 13.8ms\n",
      "Speed: 0.0ms preprocess, 13.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 14.3ms\n",
      "Speed: 2.6ms preprocess, 14.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 11.6ms\n",
      "Speed: 1.2ms preprocess, 11.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 9.6ms\n",
      "Speed: 4.0ms preprocess, 9.6ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 12.3ms\n",
      "Speed: 4.0ms preprocess, 12.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.9ms\n",
      "Speed: 0.0ms preprocess, 16.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 12.4ms\n",
      "Speed: 0.0ms preprocess, 12.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import streamlink\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "# model = YOLO(\"../yolov8n.pt\")\n",
    "# model = YOLO(\"../yolov8l.pt\")\n",
    "model = YOLO(\"../yolov8x.pt\")\n",
    "# model = YOLO(\"../yolov8n-obb.pt\")\n",
    "# model = YOLO(\"../yolov8n-seg.pt\")\n",
    "# model = YOLO(\"../yolov8l-seg.pt\")\n",
    "# model = YOLO(\"../mybest.pt\")\n",
    "# model = YOLO(\"../hatbest.pt\")\n",
    "# model = YOLO(\"../my-best-segment.pt\")\n",
    "# model = YOLO(\"../yolov10n.pt\")\n",
    "# model = YOLO(\"../yolov8n.pt\")\n",
    "# model = YOLO(\"../custom_yolov8s.pt\")\n",
    "\n",
    "\n",
    "# Open Live stream\n",
    "# url = 'https://www.youtube.com/watch?v=DjdUEyjx8GM'\n",
    "url = 'https://www.youtube.com/watch?v=gFRtAAmiFbE'\n",
    "# url = 'https://www.youtube.com/watch?v=KY4Yd5QR570'\n",
    "\n",
    "# Live streaming\n",
    "streams = streamlink.streams(url)\n",
    "# cap = cv2.VideoCapture(streams[\"360p\"].url)\n",
    "cap = cv2.VideoCapture(streams[\"1080p\"].url)\n",
    "# cap = cv2.VideoCapture(streams[\"best\"].url)\n",
    "\n",
    "# Open  video file\n",
    "# video_path = \"short_video.mp4\"\n",
    "# cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_frame_time = 0\n",
    "new_frame_time = 0\n",
    "\n",
    "# Get the video frame dimensions\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define the counting lines (adjust these values as needed)\n",
    "left_line = int(frame_width * 0.2)  # 20% from the left\n",
    "right_line = int(frame_width * 0.8)  # 20% from the right\n",
    "\n",
    "# Counters for people exiting\n",
    "left_exit_count = 0\n",
    "right_exit_count = 0\n",
    "\n",
    "# Dictionary to store track histories\n",
    "track_history = {}\n",
    "\n",
    "# Sets to store IDs of people who have left\n",
    "left_exited_ids = set()\n",
    "right_exited_ids = set()\n",
    "\n",
    "# Function to check if a person has crossed a line\n",
    "def has_crossed_line(prev_pos, curr_pos, line_pos):\n",
    "    return (prev_pos < line_pos and curr_pos >= line_pos) or (prev_pos > line_pos and curr_pos <= line_pos)\n",
    "\n",
    "def smooth_polygon(polygon, smoothing=5):\n",
    "    \"\"\"Apply smoothing to the polygon points.\"\"\"\n",
    "    smooth_polygon = np.array(polygon, dtype=np.float32)\n",
    "    for _ in range(smoothing):\n",
    "        smooth_polygon = np.array([np.mean(np.roll(smooth_polygon, shift, axis=0), axis=0)\n",
    "                                   for shift in range(-1, 2)])\n",
    "    return np.array(smooth_polygon, dtype=np.int32)\n",
    "\n",
    "\n",
    "# Frame skip variable\n",
    "frame_skip = 0 # 2 is good\n",
    "frame_counter = 0\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    # Skip frames to catch up with the stream\n",
    "    # frame_counter += 1\n",
    "    # if frame_skip > 0:\n",
    "    #     if frame_counter % frame_skip != 0:\n",
    "    #         continue\n",
    "    \n",
    "    if success:\n",
    "        \n",
    "        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n",
    "        # results = model.track(frame, persist=True, classes=0)  # class 0 is person\n",
    "        results = model.track(frame, persist=True, classes=0, conf=0.1,tracker=\"bytetrack.yaml\")  # class 0 is person, conf=0.25 sets the confidence threshold to 25%\n",
    "        # results = model.track(frame, persist=True, classes=0, conf=0.1)  # class 0 is person, conf=0.25 sets the confidence threshold to 25%\n",
    "        # results = model.track(frame, persist=True, conf=0.1)\n",
    "        # annotation\n",
    "        # frame = results[0].plot() # info from yolo v8, optional, can comment off\n",
    "        \n",
    "        # # Create a blank overlay for the semi-transparent masks\n",
    "        overlay = np.zeros_like(frame, dtype=np.uint8)\n",
    "        \n",
    "                                    # Process each detected person\n",
    "        if results[0].masks is not None:\n",
    "            for mask in results[0].masks.xy:\n",
    "                # Convert the polygon to a format suitable for cv2.fillPoly\n",
    "                polygon = np.array(mask, dtype=np.int32)\n",
    "                \n",
    "                # Fill the polygon on the overlay\n",
    "                cv2.fillPoly(overlay, [polygon], color=(255, 0, 255))  # Red color\n",
    "        \n",
    "        # Blend the overlay with the original frame\n",
    "        alpha = 0.3  # Adjust this value to change the transparency (0.0 - 1.0)\n",
    "        annotated_frame = cv2.addWeighted(frame, 1, overlay, alpha, 0)\n",
    "\n",
    "        # Draw the counting lines\n",
    "        cv2.line(annotated_frame, (left_line, 0), (left_line, frame_height), (255, 255, 0), 2)\n",
    "        cv2.line(annotated_frame, (right_line, 0), (right_line, frame_height), (255, 255, 0), 2)\n",
    "        \n",
    "        # Process each detected person\n",
    "        if results[0].boxes is not None and results[0].boxes.id is not None:\n",
    "            for box, track_id in zip(results[0].boxes.xywh, results[0].boxes.id):\n",
    "                x, y, w, h = box\n",
    "                track_id = int(track_id)\n",
    "                center_x, center_y = int(x), int(y)\n",
    "                \n",
    "                # Store the center points of each track\n",
    "                if track_id not in track_history:\n",
    "                    track_history[track_id] = []\n",
    "                track_history[track_id].append((center_x, center_y))\n",
    "                \n",
    "                # Keep only the last 30 positions\n",
    "                track_history[track_id] = track_history[track_id][-30:]\n",
    "                \n",
    "                # Check if the person has crossed a line, algo sucks....\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    prev_x = np.mean([pos[0] for pos in track_history[track_id][:-10]])\n",
    "                    curr_x = np.mean([pos[0] for pos in track_history[track_id][-10:]])\n",
    "                    \n",
    "                    if has_crossed_line(prev_x, curr_x, left_line) and track_id not in left_exited_ids:\n",
    "                        left_exit_count += 1\n",
    "                        left_exited_ids.add(track_id)\n",
    "                    elif has_crossed_line(prev_x, curr_x, right_line) and track_id not in right_exited_ids:\n",
    "                        right_exit_count += 1\n",
    "                        right_exited_ids.add(track_id)\n",
    "                \n",
    "                # Draw the track\n",
    "                if len(track_history[track_id]) > 1:\n",
    "                    cv2.polylines(annotated_frame, [np.array(track_history[track_id], dtype=np.int32)], False, (0, 255, 0), 2)\n",
    "                \n",
    "                # Draw ID near the person\n",
    "                cv2.putText(annotated_frame, f\"ID: {track_id}\", (int(x), int(y) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate FPS\n",
    "        new_frame_time = time.time()\n",
    "        fps = 1.0 / (new_frame_time - prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        \n",
    "        # Count the number of detected objects\n",
    "        num_objects = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "        \n",
    "        # Put the FPS, object count, and exit counts on the frame\n",
    "        cv2.putText(annotated_frame, f\"FPS: {int(fps)}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Detected: {num_objects}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Left exits: {left_exit_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(annotated_frame, f\"Right exits: {right_exit_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Put the \"NUS-ISS Demo\" text in the top-right corner\n",
    "        text = \"NUS-ISS Demo\"\n",
    "        text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        text_x = annotated_frame.shape[1] - text_size[0] - 10\n",
    "        text_y = 30\n",
    "        cv2.putText(annotated_frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
